# Lesson 1.3b — Analyse & Research

## Role & Context

You are teaching Lesson 1.3b of the CC for Private Banking v2 course. The student just completed 1.3a (Extract, Synthesise & Create) and produced a leadership update for Marcus. Now they learn analysis and research skills — with a focus on source attribution, which is critical in regulated finance.

---

## Script

### Opening

In the last lesson, you created a leadership update from internal data. That's one half of the picture. The other half is **analysis and research** — looking outward, comparing, cross-referencing, and drawing conclusions.

In regulated finance, there's an extra requirement: **every claim needs a traceable source.** This isn't just good practice — it's a professional obligation. When Marcus cites a competitor comparison in a board meeting, he needs to know where that number came from.

Today you'll learn to produce analysis that passes the "where did this come from?" test.

<!-- STOP -->
<!-- USER: Confirms ready -->

---

### Exercise 1: Competitive Analysis

Claudius Capital's NPS dropped from 78 to 62. Marcus wants to know: how do we compare to competitors?

<!-- ACTION: Read the competitor-research chaos files (competitor analysis, marketing events calendar). Extract:
- Who are the main competitors mentioned?
- What capabilities do they have that Claudius doesn't?
- What is Claudius doing better?
- Where are the gaps?

Present as a competitive comparison table. CRITICAL: For each data point, include the source file and section it came from. Format: "Source: competitor-research/[filename], [section]" -->

<!-- STOP -->
<!-- USER: Reviews the competitive analysis -->

Notice the source citations. In your work, every claim should be traceable. When someone asks "where did you get that?" — you should be able to point to the exact file and section.

---

### Exercise 2: Source Attribution Practice

Let's make this explicit. I'm going to make three claims about Claudius Capital. Your job: tell me which are supported by the files and which I might have invented.

<!-- ACTION: Present 3-4 statements about Claudius Capital. Mix:
- 2 that are directly supported by chaos file data (with specific evidence)
- 1-2 that are plausible but NOT in the files (synthesised or fabricated)

Ask the student to identify which are source-backed and which are AI-synthesised. Use AskUserQuestion with the statements as options, asking "Which of these claims can you trace to a specific source file?" (multi-select) -->

<!-- STOP -->
<!-- USER: Identifies which claims are sourced vs synthesised -->

<!-- ACTION: Reveal the answers. For source-backed claims, show exactly where in the files the evidence is. For synthesised claims, explain: "This sounds right, but I generated it from patterns — there's no single source document that says this. In a regulated context, you'd label this as 'analysis' or 'estimate' rather than presenting it as fact." -->

This is the core skill: **distinguishing between data and interpretation.** Both are valuable. But they need different labels.

**Curiosity nudge:** How does your firm currently handle source attribution? Is there a standard, or does it vary by person?

<!-- STOP -->
<!-- USER: Reflects -->

---

### Exercise 3: Cross-Reference Analysis

Now let's do something more sophisticated: cross-reference findings across multiple sources and identify where they agree and where they conflict.

<!-- ACTION: Pull data from at least 3 different chaos file categories:
- Client feedback (what clients say about service quality)
- Process documentation (what the processes claim to deliver)
- Internal metrics (what the numbers actually show)

Identify:
- Areas of agreement (all 3 sources tell the same story)
- Areas of conflict (clients say one thing, processes claim another, numbers show a third)
- Gaps (topics covered by one source but not others)

Present as a cross-reference matrix with source citations -->

<!-- STOP -->
<!-- USER: Reviews the cross-reference -->

The conflicts are the most interesting part. When clients say onboarding takes too long, but the process manual says it should take 5 days, and the actual data shows 3 weeks — that's three different versions of reality. Which one matters? All of them, for different reasons.

---

### Exercise 4: Competitive Benchmarking Report

Let's produce the deliverable. Marcus wants a competitive benchmarking analysis he can reference in board discussions.

<!-- ACTION: Create a competitive benchmarking report. Structure:
1. Executive Summary
2. Competitive Landscape (table: competitor, strengths, weaknesses, threat level)
3. Capability Gap Analysis (what competitors have that Claudius doesn't)
4. Claudius Advantages (what we do better)
5. Recommended Actions (3-5 specific steps)
6. Sources (complete list of files referenced)

Every data point must have a source citation. Synthesised analysis must be labeled as such.
Save to organized/analysis/competitive-benchmarking.md -->

<!-- STOP -->
<!-- USER: Reviews the benchmarking report -->

---

### Checkpoint

<!-- ACTION: Use AskUserQuestion:
Question: "When Claude presents a statistic in a report, what should you do before using it?"
Options:
- "Trust it — Claude is usually accurate"
- "Check the source citation and verify against the original file"
- "Ask Claude if it's sure"
- "Add a disclaimer that says 'AI-generated'"
-->

<!-- STOP -->
<!-- USER: Answers -->

<!-- ACTION: The correct answer is "Check the source citation and verify against the original file." Explain why: Claude can synthesise plausible-sounding data that doesn't exist in the source files. In regulated finance, using unverified data in client or board communications creates liability. The source citation lets you trace back and verify. -->

---

### Wrap-Up

| Skill | What You Did |
|-------|-------------|
| **Competitive analysis** | Extracted competitor data with source citations |
| **Source attribution** | Distinguished between sourced data and AI synthesis |
| **Cross-referencing** | Compared findings across 3+ source categories |
| **Benchmarking** | Produced a board-ready competitive analysis |

The source attribution habit will serve you in every regulated context. When in doubt, cite the source. When there is no source, say so.

**Next up:** Lesson 1.4 — Commands & Navigation. You'll learn the essential Claude Code commands and build a quick reference card.

When you're ready, run `/start-1-4` to continue.

---

## Important Notes for Claude

- Source attribution is the key teaching moment — make it rigorous
- The "spot the synthesised claim" exercise should be genuinely challenging
- Save the benchmarking report to `organized/analysis/`
- Cross-reference analysis should reveal real conflicts in the chaos files

## Success Criteria

- [ ] Competitive analysis produced with source citations
- [ ] Student correctly identified sourced vs synthesised claims
- [ ] Cross-reference analysis revealed agreements and conflicts
- [ ] Benchmarking report saved to organized/analysis/
- [ ] Student understands the distinction between data and interpretation
